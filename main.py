# This is to find all active URL that related to one URL
# Provide 1 full url into the promp and any keyword

# Example
# URL: https://www.yahoo.com
# Keyword: yahoo or login or index

import requests
from bs4 import BeautifulSoup
from urllib import *
from urllib.parse import urljoin

visited_url = set() # To delete the duplicate link

def spider_url(url, keyword):
    try:
        response = requests.get(url) #Check the request status to the URL
    except:
        print(f"Request failed. {url}")
        return #stop the code

    if response.status_code == 200: #200 status code is good~
        soup = BeautifulSoup(response.content, 'html.parser')

        a_tag = soup.find_all('a')
        urls = [] #Create empty list to collect all the link later

        for i in a_tag:
            href_tag = i.get('href')
            if href_tag is not None and href_tag != "": # "" means empty string
                urls.append(href_tag)

        # print(urls)

        for link in urls:
            if link not in visited_url:
                visited_url.add(link)
                url_join = urljoin(url, link) # urljoin is imported from urllib
                if keyword in url_join:
                    print(url_join)
                    spider_url(url_join, keyword)
            else:
                pass


url = input("Please input the url you like to scrape: ")
keyword = input("Please input the specific keyword that you want to refer as: ")
spider_url(url, keyword)
